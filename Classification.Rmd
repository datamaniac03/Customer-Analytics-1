---
title: "Customer Analytics II - Classification"
author: "Karl Melo"
date: "2016"
output: 
 html_document: 
    fig_height: 6
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
---

<style type="text/css">

body{ /* Normal  */
   font-size: 16px;
}
td {  /* Table  */
   font-size: 12px;
}
h1 { /* Header 1 */
 font-size: 28px;
 color: SteelBlue;
}
h2 { /* Header 2 */
 font-size: 22px;
 color: SteelBlue;
}
h3 { /* Header 3 */
 font-size: 18px;
 color: SteelBlue;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 12px
}
</style>

## 1. Introduction

This is the second of a series of four reports written to demonstrate the value of customer analytics to the senior management of a diversified holding company with subsidiaries in key emerging markets. The papers aim to support the argument for significant investments in data and analytics capabilities as a means of driving revenue and profit growth through improved customer relationship management.

These reports are meant for analytics practitioners.  Analyses are with the R statistical programming language.

The key insights and recommendations are presented in a separate powerpoint deck prepared for non-technical executives.

The papers implement advanced analytics methods that attempt to: 

**1. Identify managerially relevant subgroups in a customer database.**

**2. Build a model that predicts customer churn.**

**3. Build a model that predicts the spending level of customers predicted to remain loyal.**

**4. Estimate the lifetime value of each customer.**

Analyses are performed on a very narrow dataset. Although this will likely restrict the performance of the predictive models, the dataset is representative of the limited data currently available within most of our subsidiaries.

***

## 2. Objective

This report focuses on the second task - **building a binary classifier that predicts customer churn.**

The aim is to predict for each customer active in 2014, whether or not they will stay loyal in 2015.

**Performance target**

There is currently no working predictive model to serve as a baseline for the performance of the models built here.

I shall therefore use the simplest possible model as a lower bound. In this case the null model is set to predict all customers remain loyal - 0 customer churn.

Performance metrics considered will be accuracy, sensitivity and specificity.

A good model should:  

* At least outperform the baseline model.  

Statistical significance tests will be used to compare performance. 

* Meet the managerial objective determined by project sponsors.  

There is no such objective for this demonstration.

***

## 3. Set Up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
```

### Load Packages

```{r}
library(ggplot2)
library(dplyr)
library(pastecs)
library(kernlab)
library(caret)

```

### Load Data

```{r }
cust_data = read.delim(file = 'transactions.txt', header = FALSE, sep = '\t', dec = '.')

```

***

## 4. Dataset Description

The dataset is proprietary customer data consisting of 51,243 observations across 3 variables:

* customer i.d
* purchase amount in USD
* transaction date

The data covers the period January 2nd 2005 to December 31st 2015.

The dataset is tidy and ready for analysis.

**Data Preparation**

Create additional variables:

`recency` - time since last purchase (days)

`frequency` - number of purchase transactions

`avg_amount` - average amount spent

```{r}
## Name columns
cust_data <- rename(cust_data, customer_id = V1, purchase_amount = V2, date_of_purchase = V3)

## Convert purchase date to R date object
cust_data$date_of_purchase <- as.Date(cust_data$date_of_purchase, "%Y-%m-%d")

## Create recency variable
cust_data$days_since <- as.numeric(difftime(time1 = "2016-01-01",
                                            time2 = cust_data$date_of_purchase,
                                            units = "days"))   

## Create dataset for rfm analysis including frequency variable
customers <- cust_data %>%
                group_by(customer_id) %>%
                summarise(recency = min(days_since), frequency = n(), 
                          avg_amount = mean(purchase_amount))
```

***

## 5. Data Exploration

### Descriptive statistics

```{r}
options(digits = 2, scipen = 99)
stat.desc(customers)
```

### Distributions

`frequency`

The distribution is heavily right skewed. Most customers have made between 1 and 15 purchases.  A few outliers can be found beyond this point up to a maximum of 45
```{r}

ggplot(customers, aes(x = frequency)) +
   geom_histogram(bins = 20, fill = "steelblue") +
   theme_classic() +
   labs(x = "frequency", title = "Purchase frequency") +
   scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45))
```

Almost 50% of customers have only ever made 1 purchase
```{r}
dim(filter(customers, frequency == 1))[1]/dim(customers)[1]
```

`recency`

Apart from a spike on the extreme left, the distribution is fairly uniform.  
```{r}
ggplot(customers, aes(x = recency)) +
   geom_histogram(fill = "steelblue") +
   theme_classic() +
   labs(x = "recency", title = "Recency in Days") +
   scale_x_continuous(breaks = c(0, 250, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000))
```

About 30% of customers have been active in the last year.
```{r}
dim(filter(customers, recency < 365))[1]/dim(customers)[1]

x <- filter(customers, recency < 365)
```

`amount`

The series has mean 58 with 50% of samples between 22 and 50. The mean is pulled above the median by the severe right skew caused by some extreme outliers.  
```{r}
summary(customers$avg_amount)

```

```{r fig.show = 'hold', out.width = '50%'}
ggplot(customers, aes(x = avg_amount)) +
   geom_histogram(bins = 20, fill = "steelblue") +
   theme_classic() +
   labs(x = "amount", title = "average purchase amount")

ggplot(customers, aes(x = avg_amount)) +
   geom_histogram(bins = 20, fill = "steelblue") +
   theme_classic() +
   labs(x = "amount (log10)", title = "Log transformed average purchase amount") +
   scale_x_log10()
```

### Key trends

```{r}
cust_data$year_of_purchase <- as.numeric(format(cust_data$date_of_purchase, "%Y"))
```

Number of customers active in each year and corresponding total annual revenues.
```{r}
active <- cust_data %>%
                group_by(year_of_purchase) %>%
                summarise(active_customers = length(unique(customer_id)),
                           revenues = sum(purchase_amount))

print(active)
```


```{r}
ggplot(data = active, aes(x = year_of_purchase, y = active_customers)) +
  geom_line(colour = "darkred", size = 1.5) +
  theme_gray() +
  labs(x = "Year", y = "No. of Customers", title = "Customers")
```

```{r}
ggplot(data = active, aes(x = year_of_purchase, y = revenues/1000)) +
  geom_line(colour = "darkred", size = 1.5) +
  theme_gray() +
  labs(x = "Year", y = "Revenues\n(thousands)", title = "Revenues") 
  
```

***

## 6. Modelling

### Objective

The objective is to assign customers into 1 of 2 categories (churn/no churn).

There are scores of algorithms that attempt to model a binary response from a given set of features. Model selection is a challenge that requires skill, experience and a fair bit trial and error.

### Strategy

I will first set a performance ceiling by fitting some flexible, non-linear models. They generally make up for what they lack in interpretability with better predictive performance.  Seeing as the principal goal here is predictive accuracy, the performance of these models will serve as a useful guide.

Next I will try some simpler, more interpretable linear models to see if their performance is acceptably close to the non-linear models.

Predictive accuracy and simplicity is after all the ideal in a statistical model. 

The 'best' model will then be compared to the baseline model described above. If my best model performs significantly better than the baseline, I will investigate the feasibility of further tuning to approach the bayes model; the most accurate possible model given the available set of predictors.

Results are evaluated in section 7.

Models are fitted on a dataset made up of both customers active in 2014 and 2015 (did not churn) and those active in 2014 but not so in 2015 (churned). Any customers acquired in 2015 will therefore not be allowed to interefere with the analysis. 

### Data pre-processing

Customers active in 2014
```{r}

active_2014 <- cust_data %>%
                    filter(year_of_purchase == 2014) %>%
                       group_by(customer_id) %>%
                          summarise(recency = min(days_since), 
                                    frequency = n(), 
                                    avg_amount = mean(purchase_amount),
                                    first_purchase = max(days_since),
                                    max_amount = max(purchase_amount))
                  

summary(active_2014)
```


Customers active in 2015
```{r}
active_2015 <- cust_data %>%
                    filter(year_of_purchase == 2015) %>%
                       group_by(customer_id) %>%
                          summarise(recency = min(days_since), 
                                    frequency = n(), 
                                    avg_amount = mean(purchase_amount),
                                    first_purchase = max(days_since),
                                    max_amount = max(purchase_amount))

summary(active_2015)
```

New customers acquired in 2015
```{r}

new_cust2015 <- cust_data %>%
                     group_by(customer_id) %>%
                     summarise(first_purchase = max(days_since)) %>%
                     filter(first_purchase <= 365)
                     
```

Identify customers present in `active_2014` but not in `active_2015` - the churners.
```{r}

churners <- anti_join(active_2014, active_2015, by = "customer_id")
```

Note there are some customers who were active in 2015 althought not so in 2014 but who were not newly acquired in 2015.  They are old customers who were active before 2014, skipped that year and then returned in 2015. They are excluded from the model data.

Label customers in `active_2014` to create the binary response variable - $y = churn/loyal$. 

**Dataset used for model fitting**
```{r}

model_data <- mutate(active_2014, 
                     churn = ifelse(active_2014$customer_id %in% churners$customer_id, 
                                    "churn", "loyal"))

model_data$churn <- factor(model_data$churn)

row.names(model_data) <- model_data$customer_id

model_data <- select(model_data, -customer_id)
```

Set `customer_id` as row names and remove customer_id variable.
```{r}
row.names(model_data) <- model_data$customer_id
model_data <- model_data[,-1]

```


Response variable

`churn`

Predictors

`recency`, `first_purchase`, `frequency`, `av_amount`


**High churn rate - 40%**
```{r}
table(model_data$churn)
dim(filter(model_data, churn == "churn"))[1]/dim(model_data)[1]
```

**Partition data into training and test sets**

Create balanced data splits
```{r}
set.seed(11)
trainIndex <- createDataPartition(model_data$churn, p = .75,
                                  list = FALSE,
                                  times = 1)
head(trainIndex)
```

```{r}
train_data <- model_data[ trainIndex,]
test_data  <- model_data[-trainIndex,]
```

### Model Fitting

**Non-linear models**

*Random forests*

Cross validation settings
```{r}

ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 5, 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

Model fit
```{r cache=TRUE}
set.seed(12)
rF.fit <- train(churn ~., 
            data = train_data, 
            method = "rf", 
            metric = "ROC", 
            trControl = ctrl)

```

*Support Vector Machines*

Cross validation settings
```{r}
set.seed(13)
sigmaRange <- sigest(churn ~., data = train_data)
svmRGrid <- expand.grid(.sigma = sigmaRange,
                               .C = 2^(seq(-4, 4)))
```

Model fit
```{r cache=TRUE}

set.seed(12)
svm.fit <- train(churn ~., data = train_data,
                 method = "svmRadial",
                 metric = "ROC",
                 preProc = c("center", "scale"),
                 tuneGrid = svmRGrid,
                 fit = FALSE,
                 trControl = ctrl)

```


*K-Nearest Neighbour*

Model fit
```{r cache=TRUE}
set.seed(12)
knn.fit <- train(churn ~ ., data=train_data, method="knn",
                trControl=ctrl, metric="ROC", tuneLength=20,
                preProc=c("center", "scale"))
```


**Linear models**

Cross validation settings for both linear models
```{r}

ctrl_lM <- trainControl(method = "LGOCV",
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE)
```

*Generalized linear model*
```{r cache=TRUE}
set.seed(12)
lM.fit <- train(churn ~ .,
            data = train_data,
            method = "glm",
            metric = "ROC",
            trControl = ctrl_lM)
      
```

*Linear discriminant analysis*
```{r cache=TRUE}
set.seed(12)
lda.fit <- train(churn ~ .,
                 data = train_data,
                 method = "lda",
                 metric = "ROC",
                 preProc = c("center", "scale"),
                 trControl = ctrl_lM)
```

### Model selection

**Compare non-linear models**

```{r}
model_results1 <- resamples(list(rF = rF.fit,
                                svm = svm.fit,
                                knn = knn.fit))

summary(model_results1)
```

Visualize results
```{r fig.width=12}
bwplot(model_results1, 
       fill = "steelblue",
       main = "Comparative performance of non-linear models")
```


**Compare linear models**
```{r}
model_results2 <- resamples(list(glm = lM.fit,
                                 lda = lda.fit))

summary(model_results2)
```


**Apply selected model to the test set**

Select *random forest* and use to predict on the test set
```{r}
rFPred <- predict(rF.fit, test_data)
```

```{r}
cMatrix <- confusionMatrix(rFPred, test_data$churn)
print(cMatrix)
```

***

## 7. Evaluation and critique

### Summary

Although the results vary modestly across the different models, none of these classifiers satisfy the performance criteria set out in section 2.

Of my 5 classifiers, the random forest model performs the best overall, although the knn and support vector machines perform better on one metric or the other.

The linear models perform similarly to the non-linear ones on ROC but very poorly on sensitivity, arguably the most important metric in a context such as ours.

Results are summarized in the tables and plot above.

### Comparison to the null model

The null classifies all customers as loyal - it predicts 0 churn. Given our known churn rate of 40%, this model will have an overall error rate of 40%.

The random forest model has a cross validation error rate of 38%. Barely below the null model.

```{r}
rF.fit$finalModel
```

However, it is important to note that the null model, always predicting no customer churns, will have sensitivity 0 compared to the random forest model's 38.4% on the test set. Nevertheless, 38.4% sensitivity despite being much better than the null model is not very impressive in this context.

My best model slightly outperforms the null model. It offers modest value greater than the status quo.

### Critique

This modest performance could either be due to deficiencies in the models or in the data.

**Possible model deficiencies**

Classifiers could perform poorly due to a number of model related reasons:

* **Mismatches in the inductive biases of particular models and the data.** For example linear models that assume linear relationships between predictors and the response variable will struggle to correctly identify non-linear decision boundaries in a feature space.

*In addition to simple linear models, I applied some flexible models such as a tree model and a support vector machine with reputations for accurately modelling complex, non-linear relationships.*

* **Poor parameter tuning** 

* **Outliers** are known to strongly prejudice some models. 

Data exploration (part 5) do indeed show the presence of unusual values.  However, 

In addition to the steps described above, the fact that the key performance metrics are broadly similar across the very different types of models implemented here suggests the underperformance is most likely due to deficiencies in the data.

**Data deficiencies**

*Data Quality*

Innacurate or inconsistently recorded data can have dramatic effects on predictive models.  I have no reason to suspect the quality of the data used here.

*Feature space*

My classifiers rely on a very narrow feature space to predict classes. 4 input variables.  

Context: for the Kaggle Cup 2009 data science competition, the telecommunication company Orange France provided 230 features to serve as inputs in classification models predicting customer churn.

It is more than likely that our 4 features do not sufficiently explain the variation observed in the response.

My conclusion is that the models' relatively poor performance is mainly due to insufficient data.


***

## 8. Conclusion

One of the key objectives of this series of papers is to motivate a data collection and management program at some of our subsidiaries. 

The models presented here can be highly predictive.  Some of them are highly sophisticated and are capable of drawing complex decision boundaries to assign non-linear data to separate classes.

However, they require data to be effective.  In most cases, the more the better. If input variables can be combined to explain variation in a response, it stands to reason that larger numbers of informative inputs will lead to more accurate predictions in most cases.



